{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen Core - Distributed\n",
    "\n",
    "I'm only going to give a Teaser of this!!\n",
    "\n",
    "Partly because I'm unsure how relevant it is to you. If you'd like me to add more content for this, please do let me know.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ALL_IN_ONE_WORKER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with our Message class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - a host for our distributed runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GrpcWorkerAgentRuntimeHost class, which provides the gRPC-based host for distributed agent runtimes\n",
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n",
    "\n",
    "# Create an instance of the gRPC host, specifying the address (host:port) it should listen on for worker connections\n",
    "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
    "\n",
    "# Start the host service so it begins listening for incoming gRPC connections from worker runtimes\n",
    "host.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's reintroduce a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GoogleSerperAPIWrapper, which provides a search interface using the Serper API\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "\n",
    "# Wrap the Serper API as a LangChain Tool, giving it a name and description for agent use\n",
    "langchain_serper = Tool(\n",
    "    name=\"internet_search\", \n",
    "    func=serper.run, \n",
    "    description=\"Useful for when you need to search the internet\"\n",
    ")\n",
    "\n",
    "# Adapt the LangChain Tool for use with AutoGen agents by wrapping it with LangChainToolAdapter\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction1 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons in favor of choosing AutoGen; the pros of AutoGen.\"\n",
    "\n",
    "instruction2 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons against choosing AutoGen; the cons of Autogen.\"\n",
    "\n",
    "judge = \"You must make a decision on whether to use AutoGen for a project. \\\n",
    "Your research team has come up with the following reasons for and against. \\\n",
    "Based purely on the research from your team, please respond with your decision and brief rationale.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And make some Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Judge(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        message1 = Message(content=instruction1)\n",
    "        message2 = Message(content=instruction2)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message1, inner_1)\n",
    "        response2 = await self.send_message(message2, inner_2)\n",
    "        result = f\"## Pros of AutoGen:\\n{response1.content}\\n\\n## Cons of AutoGen:\\n{response2.content}\\n\\n\"\n",
    "        judgement = f\"{judge}\\n{result}Respond with your decision and brief explanation\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime  # Import the gRPC-based worker runtime for agent execution\n",
    "\n",
    "if ALL_IN_ONE_WORKER:  # Check if all agents should be run in a single worker process\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")  # Create a worker runtime connected to the host at the specified address\n",
    "    await worker.start()  # Start the worker runtime asynchronously\n",
    "\n",
    "    await Player1Agent.register(worker, \"player1\", lambda: Player1Agent(\"player1\"))  # Register Player1Agent with the worker under the name \"player1\"\n",
    "    await Player2Agent.register(worker, \"player2\", lambda: Player2Agent(\"player2\"))  # Register Player2Agent with the worker under the name \"player2\"\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))  # Register Judge agent with the worker under the name \"judge\"\n",
    "\n",
    "    agent_id = AgentId(\"judge\", \"default\")  # Create an AgentId for the judge agent in the default namespace\n",
    "\n",
    "else:  # If agents should be distributed across multiple workers\n",
    "\n",
    "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")  # Create the first worker runtime for player1\n",
    "    await worker1.start()  # Start the first worker asynchronously\n",
    "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))  # Register Player1Agent with worker1\n",
    "\n",
    "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")  # Create the second worker runtime for player2\n",
    "    await worker2.start()  # Start the second worker asynchronously\n",
    "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))  # Register Player2Agent with worker2\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")  # Create a separate worker runtime for the judge\n",
    "    await worker.start()  # Start the judge's worker asynchronously\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))  # Register Judge agent with its worker\n",
    "    agent_id = AgentId(\"judge\", \"default\")  # Create an AgentId for the judge agent in the default namespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await worker.send_message(Message(content=\"Go!\"), agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pros of AutoGen:\n",
       "Here are several pros of using AutoGen in your new AI Agent project:\n",
       "\n",
       "1. **Multi-Agent Collaboration**: AutoGen excels at facilitating interaction among multiple agents, allowing them to collaborate effectively on complex tasks, which can lead to better outcomes compared to single-agent systems.\n",
       "\n",
       "2. **Efficiency**: It streamlines complex tasks and automates workflows, making it easier to manage and execute intricate processes through the cooperation of autonomous agents.\n",
       "\n",
       "3. **Scalability**: AutoGen's architecture allows agents to handle multiple tasks simultaneously, making it highly scalable and suitable for projects with growing demands.\n",
       "\n",
       "4. **Optimization with LLMs**: AutoGen can integrate with various large language models (LLMs), enhancing the problem-solving capabilities of the agents by leveraging specialized tools tailored for different tasks.\n",
       "\n",
       "5. **Customization and Adaptability**: The agents created using AutoGen can be tailored to specific needs and can adapt based on ongoing tasks or human feedback, ensuring flexibility in diverse applications.\n",
       "\n",
       "6. **Dynamic Collaboration**: AutoGen fosters an environment where agents can develop collaborative strategies, which enhances their performance and ability to tackle more complicated issues than individual agents might manage alone.\n",
       "\n",
       "These features make AutoGen a compelling choice for projects requiring sophisticated AI agent capabilities. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "## Cons of AutoGen:\n",
       "Here are some cons of using AutoGen for your AI agent project:\n",
       "\n",
       "1. **Complexity and Learning Curve**: AutoGen requires a solid understanding of multi-agent systems, which can be a significant barrier for newcomers or those without programming expertise.\n",
       "\n",
       "2. **Challenging Documentation**: The documentation for AutoGen is often described as hard to read with insufficient examples, making it difficult for users to learn and utilize the framework effectively.\n",
       "\n",
       "3. **Customization Complexity**: While AutoGen offers extensive customization options, this flexibility can also lead to increased complexity, possibly overwhelming developers who are new to such extensive configuration.\n",
       "\n",
       "4. **Interaction Management**: AutoGen lacks an inherent concept of process management, which can complicate handling interactions among agents within the system, leading to potential confusion or errors in coordination.\n",
       "\n",
       "These factors could be critical in deciding whether AutoGen is the right fit for your project. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "\n",
       "\n",
       "## Decision:\n",
       "\n",
       "Based on the research from the team, I recommend using AutoGen for the project. The advantages of multi-agent collaboration, efficiency, scalability, optimization with LLMs, customization, and dynamic interaction outweigh the challenges of complexity and documentation issues. The ability to tackle complex tasks through collaboration among agents is particularly compelling for sophisticated AI capabilities. While there may be a learning curve, the potential for enhanced outcomes justifies the initial investment in usability training and overcoming documentation challenges.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "await worker.stop()\n",
    "if not ALL_IN_ONE_WORKER:\n",
    "    await worker1.stop()\n",
    "    await worker2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "await host.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
